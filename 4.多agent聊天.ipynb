{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGEN 第四课： 多agent聊天\n",
    "\n",
    "之前的例子中，我们都是通过2个agent的对话来解决一些问题，如果遇到更复杂的任务，可能就需要用到多agent的场景了。\n",
    "接下来，我们介绍一种简单的多agent模式。\n",
    "\n",
    "1. 我们构造assistant agent 名叫planner和user proxy agent: planner_user。为了方便期间，我们另planner_user 的 human_input_mode=\"NEVER\".\n",
    "2. 构造一个ask_planner函数，注意， 这里的函数并不是调用外部的函数，而是为了让这2个agent开始对话，并返回最后一轮的结果。\n",
    "3. 我们在构造一个常规的2个agent，并发起对话，并在合适的时候调用ask_planner函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置环境\n",
    "\n",
    "1. 你需要安装autogent的库， 要求python版本>=3.8\n",
    "2. 配置openai 或者 azure openai的api key, 并将key写入到一个名为 \"OAI_CONFIG_LIST\"的文件中，并将该文件放在项目目录下。或者将其配置到你的环境变量中。\n",
    "\n",
    "OAI_CONFIG_LIST的内容结构如下：\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyautogen~=0.1.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.将2个agent的对话过程封装成一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"planner\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=\"You are a helpful AI assistant. You suggest coding and reasoning steps for another AI assistant to accomplish a task. Do not suggest concrete code. For any action beyond writing code or reasoning, convert it to a step which can be implemented by writing code. For example, the action of browsing the web can be implemented by writing code which reads and prints the content of a web page. Finally, inspect the execution result. If the plan is not good, suggest a better plan. If the execution is wrong, analyze the error and suggest a fix.\"\n",
    ")\n",
    "\n",
    "planner_user = autogen.UserProxyAgent(\n",
    "    name=\"planner_user\",\n",
    "    max_consecutive_auto_reply=0,  \n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "def ask_planner(message):\n",
    "    planner_user.initiate_chat(planner, message=message)\n",
    "    # 返回从planner收到的最后一轮信息\n",
    "    return planner_user.last_message()[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Suggest a fix to an open good first issue of flaml\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To suggest a fix to an open good first issue of FLAML, we first need to fetch the list of open issues labeled as \"good first issue\" from the FLAML GitHub repository. We can do this using the GitHub API.\n",
      "\n",
      "Here is a Python script that uses the requests library to fetch the list of open issues labeled as \"good first issue\" from the FLAML GitHub repository.\n",
      "\n",
      "```python\n",
      "# filename: fetch_issues.py\n",
      "\n",
      "import requests\n",
      "import json\n",
      "\n",
      "def fetch_issues():\n",
      "    url = \"https://api.github.com/repos/microsoft/FLAML/issues\"\n",
      "    params = {\n",
      "        \"state\": \"open\",\n",
      "        \"labels\": \"good first issue\"\n",
      "    }\n",
      "    response = requests.get(url, params=params)\n",
      "    issues = json.loads(response.text)\n",
      "    for issue in issues:\n",
      "        print(f\"Issue ID: {issue['id']}, Title: {issue['title']}, URL: {issue['html_url']}\")\n",
      "\n",
      "fetch_issues()\n",
      "```\n",
      "\n",
      "Please run this script to fetch the list of open issues. After that, we can select one issue and suggest a fix for it.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"fetch_issues.py\", line 17, in <module>\n",
      "    fetch_issues()\n",
      "  File \"fetch_issues.py\", line 15, in fetch_issues\n",
      "    print(f\"Issue ID: {issue['id']}, Title: {issue['title']}, URL: {issue['html_url']}\")\n",
      "TypeError: string indices must be integers\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I apologize for the confusion. The error is due to the fact that the response from the GitHub API is not as expected. It seems that the response is not a list of issues but a single string, which is causing the TypeError.\n",
      "\n",
      "Let's modify the code to print the response from the API to understand the structure of the response.\n",
      "\n",
      "```python\n",
      "# filename: fetch_issues.py\n",
      "\n",
      "import requests\n",
      "import json\n",
      "\n",
      "def fetch_issues():\n",
      "    url = \"https://api.github.com/repos/microsoft/FLAML/issues\"\n",
      "    params = {\n",
      "        \"state\": \"open\",\n",
      "        \"labels\": \"good first issue\"\n",
      "    }\n",
      "    response = requests.get(url, params=params)\n",
      "    print(response.text)\n",
      "\n",
      "fetch_issues()\n",
      "```\n",
      "\n",
      "Please run this script to print the response from the GitHub API.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "{\"message\":\"API rate limit exceeded for 209.141.52.127. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\",\"documentation_url\":\"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"}\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The error message indicates that the API rate limit has been exceeded for the IP address from which the request is being made. GitHub imposes a rate limit on unauthenticated requests to prevent abuse.\n",
      "\n",
      "To get around this, we can authenticate our requests using a GitHub token. However, sharing a GitHub token here is not secure and not recommended.\n",
      "\n",
      "Instead, I suggest you manually visit the FLAML GitHub repository's issues page and filter by \"good first issue\". Here is the URL: https://github.com/microsoft/FLAML/issues\n",
      "\n",
      "Once you've selected an issue, I can help you understand it and suggest a potential fix.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: ask_planner *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"message\": \"The user wants to fix an open good first issue of FLAML. However, due to GitHub API rate limit, we couldn't fetch the issues programmatically. The user has been advised to manually visit the FLAML GitHub repository's issues page and filter by 'good first issue'. What should be the next step?\"\n",
      "}\n",
      "\u001b[32m************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ask_planner...\u001b[0m\n",
      "\u001b[33mplanner_user\u001b[0m (to planner):\n",
      "\n",
      "The user wants to fix an open good first issue of FLAML. However, due to GitHub API rate limit, we couldn't fetch the issues programmatically. The user has been advised to manually visit the FLAML GitHub repository's issues page and filter by 'good first issue'. What should be the next step?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mplanner\u001b[0m (to planner_user):\n",
      "\n",
      "The next step would be their selection of a 'good first issue' they are interested in fixing. From there on, the steps can be as follows:\n",
      "\n",
      "1. The user should be guided to clone the FLAML repository to their local machine. This would be accomplished by writing a code to execute git clone with the FLAML's repository URL.\n",
      "\n",
      "2. Instruct the user to create a new branch for the issue they are going to work on. This can be done by writing a code that performs a 'git checkout -b' operation with a relevant branch name.\n",
      "\n",
      "3. The user then should be advised to install the project and its dependencies locally. This can be done by guiding the user to execute the appropriate commands to install the necessary requirements, which can be outlined in the project's README file.\n",
      "\n",
      "4. Next, guide the user to locate the files that need correction. They should update the code to rectify the issue, regularly commit the changes, and push them to their branch. Don't forget to write meaningful commit messages when incorporating these changes.\n",
      "\n",
      "5. Once all the corrections have been made, the user should be instructed to run the project's test suite locally to ensure the modifications haven't introduced new issues. This can also be done by following commands in the project's README file.\n",
      "\n",
      "6. If the tests pass successfully, the user should create a pull request on GitHub to merge their fixes into the main branch of the FLAML repository. They can do this either manually through the GitHub interface or by writing a script to submit a pull request via the GitHub API.\n",
      "\n",
      "7. Lastly, guide the user to provide a detailed description while opening the pull request. This should include the issue number it fixes, a summary of the changes made, and the testing strategy followed.\n",
      "\n",
      "As an AI, you should continually guide, check, and validate each step in the programming and reasoning process, and provide feedback and support as necessary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"ask_planner\" *****\u001b[0m\n",
      "The next step would be their selection of a 'good first issue' they are interested in fixing. From there on, the steps can be as follows:\n",
      "\n",
      "1. The user should be guided to clone the FLAML repository to their local machine. This would be accomplished by writing a code to execute git clone with the FLAML's repository URL.\n",
      "\n",
      "2. Instruct the user to create a new branch for the issue they are going to work on. This can be done by writing a code that performs a 'git checkout -b' operation with a relevant branch name.\n",
      "\n",
      "3. The user then should be advised to install the project and its dependencies locally. This can be done by guiding the user to execute the appropriate commands to install the necessary requirements, which can be outlined in the project's README file.\n",
      "\n",
      "4. Next, guide the user to locate the files that need correction. They should update the code to rectify the issue, regularly commit the changes, and push them to their branch. Don't forget to write meaningful commit messages when incorporating these changes.\n",
      "\n",
      "5. Once all the corrections have been made, the user should be instructed to run the project's test suite locally to ensure the modifications haven't introduced new issues. This can also be done by following commands in the project's README file.\n",
      "\n",
      "6. If the tests pass successfully, the user should create a pull request on GitHub to merge their fixes into the main branch of the FLAML repository. They can do this either manually through the GitHub interface or by writing a script to submit a pull request via the GitHub API.\n",
      "\n",
      "7. Lastly, guide the user to provide a detailed description while opening the pull request. This should include the issue number it fixes, a summary of the changes made, and the testing strategy followed.\n",
      "\n",
      "As an AI, you should continually guide, check, and validate each step in the programming and reasoning process, and provide feedback and support as necessary.\n",
      "\u001b[32m********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Based on the plan, here are the steps you need to follow:\n",
      "\n",
      "1. Clone the FLAML repository to your local machine. You can do this by running the following command in your terminal:\n",
      "\n",
      "```sh\n",
      "git clone https://github.com/microsoft/FLAML.git\n",
      "```\n",
      "\n",
      "2. Create a new branch for the issue you are going to work on. Replace `<branch-name>` with a relevant name for your branch:\n",
      "\n",
      "```sh\n",
      "cd FLAML\n",
      "git checkout -b <branch-name>\n",
      "```\n",
      "\n",
      "3. Install the project and its dependencies locally. The necessary commands to install the requirements are outlined in the project's README file. Usually, it involves installing the requirements using pip:\n",
      "\n",
      "```sh\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "4. Locate the files that need correction and update the code to rectify the issue. Regularly commit the changes and push them to your branch. Remember to write meaningful commit messages when incorporating these changes.\n",
      "\n",
      "5. Once all the corrections have been made, run the project's test suite locally to ensure the modifications haven't introduced new issues. This can also be done by following commands in the project's README file.\n",
      "\n",
      "6. If the tests pass successfully, create a pull request on GitHub to merge your fixes into the main branch of the FLAML repository. You can do this manually through the GitHub interface.\n",
      "\n",
      "7. Provide a detailed description while opening the pull request. This should include the issue number it fixes, a summary of the changes made, and the testing strategy followed.\n",
      "\n",
      "As you progress through these steps, feel free to ask for guidance or clarification at any point.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.多agent聊天.ipynb 单元格 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m assistant \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mAssistantAgent(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     llm_config\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m user_proxy \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mUserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muser_proxy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     human_input_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTERMINATE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     function_map\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mask_planner\u001b[39m\u001b[39m\"\u001b[39m: ask_planner},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     assistant,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39mSuggest a fix to an open good first issue of flaml\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hupan/Codes/KDF/gitcodes/AI-Agent/4.%E5%A4%9Aagent%E8%81%8A%E5%A4%A9.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "    \u001b[0;31m[... skipping similar frames: ConversableAgent.send at line 334 (7 times), ConversableAgent.receive at line 464 (6 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:637\u001b[0m, in \u001b[0;36mConversableAgent.generate_code_execution_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m# found code blocks, execute code and push \"last_n_messages\" back\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m exitcode, logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_code_blocks(code_blocks)\n\u001b[1;32m    638\u001b[0m code_execution_config[\u001b[39m\"\u001b[39m\u001b[39mlast_n_messages\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m last_n_messages\n\u001b[1;32m    639\u001b[0m exitcode2str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexecution succeeded\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m exitcode \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mexecution failed\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:902\u001b[0m, in \u001b[0;36mConversableAgent.execute_code_blocks\u001b[0;34m(self, code_blocks)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m    895\u001b[0m     colored(\n\u001b[1;32m    896\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m>>>>>>>> EXECUTING CODE BLOCK \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m (inferred language is \u001b[39m\u001b[39m{\u001b[39;00mlang\u001b[39m}\u001b[39;00m\u001b[39m)...\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m     flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    900\u001b[0m )\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m lang \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mshell\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msh\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 902\u001b[0m     exitcode, logs, image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_code(code, lang\u001b[39m=\u001b[39;49mlang, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_code_execution_config)\n\u001b[1;32m    903\u001b[0m \u001b[39melif\u001b[39;00m lang \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPython\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    904\u001b[0m     \u001b[39mif\u001b[39;00m code\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m# filename: \u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:885\u001b[0m, in \u001b[0;36mConversableAgent.run_code\u001b[0;34m(self, code, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_code\u001b[39m(\u001b[39mself\u001b[39m, code, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    872\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run the code and return the result.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \n\u001b[1;32m    874\u001b[0m \u001b[39m    Override this function to modify the way to run the code.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39m        image (str or None): the docker image used for the code execution.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mreturn\u001b[39;00m execute_code(code, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/site-packages/autogen/code_utils.py:314\u001b[0m, in \u001b[0;36mexecute_code\u001b[0;34m(code, timeout, filename, work_dir, use_docker, lang)\u001b[0m\n\u001b[1;32m    312\u001b[0m     signal\u001b[39m.\u001b[39malarm(timeout)\n\u001b[1;32m    313\u001b[0m     \u001b[39m# run the code in a subprocess in the current docker container in the working directory\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     result \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    315\u001b[0m         cmd,\n\u001b[1;32m    316\u001b[0m         cwd\u001b[39m=\u001b[39;49mwork_dir,\n\u001b[1;32m    317\u001b[0m         capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    318\u001b[0m         text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    320\u001b[0m     signal\u001b[39m.\u001b[39malarm(\u001b[39m0\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    506\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[39m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   1155\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/subprocess.py:2005\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1998\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   1999\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2000\u001b[0m                         skip_check_and_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   2001\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(  \u001b[39m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2003\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfailed to raise TimeoutExpired.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2005\u001b[0m ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m   2006\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2008\u001b[0m \u001b[39m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m \u001b[39m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/anaconda3/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"temperature\": 0,\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"functions\": [\n",
    "            {\n",
    "                \"name\": \"ask_planner\",\n",
    "                \"description\": \"ask planner to: 1. get a plan for finishing a task, 2. verify the execution result of the plan and potentially suggest new plan.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"message\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"question to ask planner. Make sure the question include enough context, such as the code and the execution result. The planner does not know the conversation between you and the user, unless you share the conversation with the planner.\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"message\"],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: \"content\" in x and x[\"content\"] is not None and x[\"content\"].rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    function_map={\"ask_planner\": ask_planner},\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"Suggest a fix to an open good first issue of flaml\"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
